{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/qiujiang/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 19:37:46 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:37:46,922\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "[2025-02-13 19:37:51] server_args=ServerArgs(model_path='Qwen/Qwen2-7B', tokenizer_path='Qwen/Qwen2-7B', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='float16', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen2-7B', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='127.0.0.1', port=30000, mem_fraction_static=0.8, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, stream_output=False, random_seed=1234, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=2, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False)\n",
      "[2025-02-13 19:37:52] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 19:37:56 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 02-13 19:37:56 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-13 19:37:58 TP0] Casting torch.bfloat16 to torch.float16.\n",
      "[2025-02-13 19:37:58 TP0] Casting torch.bfloat16 to torch.float16.\n",
      "[2025-02-13 19:37:58 TP0] Init torch distributed begin.\n",
      "[2025-02-13 19:37:59 TP0] Load weight begin. avail mem=78.84 GB\n",
      "[2025-02-13 19:37:59 TP0] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.76s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.68s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.75s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.78s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.76s/it]\n",
      "\n",
      "[2025-02-13 19:38:14 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.float16, avail mem=64.42 GB\n",
      "[2025-02-13 19:38:15 TP0] KV Cache is allocated. K size: 24.32 GB, V size: 24.32 GB.\n",
      "[2025-02-13 19:38:15 TP0] Memory pool end. avail mem=14.00 GB\n",
      "[2025-02-13 19:38:15 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.67it/s]\n",
      "[2025-02-13 19:38:21 TP0] Capture cuda graph end. Time elapsed: 6.27 s\n",
      "[2025-02-13 19:38:21 TP0] max_total_num_tokens=910891, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=3559, context_len=131072\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sglang as sgl\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,6,7\"\n",
    "model_args = {\n",
    "            \"model_path\": \"Qwen/Qwen2-7B\",\n",
    "            \"mem_fraction_static\": 0.8,\n",
    "            \"trust_remote_code\": False,\n",
    "            \"dtype\": 'float16',\n",
    "            \"device\": \"cuda\",\n",
    "            \"random_seed\": 1234,\n",
    "            \"base_gpu_id\": 2,\n",
    "            \"load_format\": 'auto',\n",
    "            \"context_length\": None,\n",
    "            \"dp_size\": 1,\n",
    "            \"tp_size\": 1,\n",
    "            \"log_level\": \"info\",\n",
    "        }\n",
    "\n",
    "sgl_llm = sgl.Engine(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-13 19:38:25 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-13 19:38:32 config.py:542] This model supports multiple tasks: {'generate', 'classify', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 02-13 19:38:32 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-13 19:38:32 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 02-13 19:38:32 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2-7B', speculative_config=None, tokenizer='Qwen/Qwen2-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config=None, tokenizer_revision=main, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=Qwen/Qwen2-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-13 19:38:32 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-13 19:38:33 model_runner.py:1110] Starting to load model Qwen/Qwen2-7B...\n",
      "INFO 02-13 19:38:33 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.04it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.01it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.05s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.03s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 19:38:38 model_runner.py:1115] Loading model weights took 14.2717 GB\n",
      "INFO 02-13 19:38:39 worker.py:267] Memory profiling takes 0.66 seconds\n",
      "INFO 02-13 19:38:39 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.80) = 63.40GiB\n",
      "INFO 02-13 19:38:39 worker.py:267] model weights take 14.27GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 47.64GiB.\n",
      "INFO 02-13 19:38:39 executor_base.py:110] # CUDA blocks: 55749, # CPU blocks: 4681\n",
      "INFO 02-13 19:38:39 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 6.81x\n",
      "INFO 02-13 19:38:42 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 19:38:57 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.19 GiB\n",
      "INFO 02-13 19:38:57 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 19.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_args = {\n",
    "            'model': 'Qwen/Qwen2-7B',\n",
    "            'gpu_memory_utilization': 0.8,\n",
    "            'trust_remote_code': False, \n",
    "            'revision': 'main',\n",
    "            'dtype': 'float16', \n",
    "            'seed': 1234,\n",
    "            'max_model_len': None, \n",
    "            'tensor_parallel_size': 1, \n",
    "            'pipeline_parallel_size': 1, \n",
    "            'swap_space': 4, \n",
    "        }\n",
    "\n",
    "vllm_llm = LLM(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sglang.srt.hf_transformers_utils import get_tokenizer\n",
    "sgl_tokenizer = get_tokenizer(\n",
    "               \"Qwen/Qwen2-7B\",\n",
    "               tokenizer_mode=\"auto\",\n",
    "               trust_remote_code=False,\n",
    "               tokenizer_revision=\"main\",\n",
    "            )\n",
    "sgl_sampling_params = {\n",
    "                  'max_new_tokens': 256, \n",
    "                  'stop_token_ids': [], \n",
    "                  'temperature': 1.0, \n",
    "                  'stop': ['Question=', 'Question', '=', '<|endoftext|>'], \n",
    "                  'top_p': 1.0, \n",
    "                  'top_k': -1, \n",
    "                  'min_p': 0.0, \n",
    "                  'frequency_penalty': 0.0, \n",
    "                  'presence_penalty': 0.0, \n",
    "                  'repetition_penalty': 1.0, \n",
    "                  'min_new_tokens': 0, \n",
    "                  'skip_special_tokens': True,\n",
    "                  'n': 1\n",
    "               }\n",
    "\n",
    "from vllm.transformers_utils.tokenizer import get_tokenizer\n",
    "vllm_tokenizer = get_tokenizer(\n",
    "               \"Qwen/Qwen2-7B\",\n",
    "               tokenizer_mode=\"auto\",\n",
    "               trust_remote_code=False,\n",
    "               tokenizer_revision=\"main\",\n",
    "            )\n",
    "\n",
    "vllm_sampling_params = SamplingParams(\n",
    "      n=1, \n",
    "      presence_penalty=0.0, \n",
    "      frequency_penalty=0.0, \n",
    "      repetition_penalty=1.0, \n",
    "      temperature=1.0, \n",
    "      top_p=1.0, \n",
    "      top_k=-1, \n",
    "      min_p=0.0, \n",
    "      seed=None, \n",
    "      stop=['Question=', 'Question', '=', '<|endoftext|>'], \n",
    "      stop_token_ids=[], \n",
    "      bad_words=[], \n",
    "      include_stop_str_in_output=False, \n",
    "      ignore_eos=False, \n",
    "      max_tokens=256, \n",
    "      min_tokens=0, \n",
    "      logprobs=0, \n",
    "      prompt_logprobs=None, \n",
    "      skip_special_tokens=True, \n",
    "      spaces_between_special_tokens=True, \n",
    "      truncate_prompt_tokens=None, \n",
    "      guided_decoding=None\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这些text总有对齐的时候....，我尝试总结规律，失败了\n",
    "input_text = \"Question: Fred was preparing for a party to be held in four days.  So, he made 24 gallons of root beer on the first day and put them in the refrigerator cooler.  But later that evening, his children discovered the delicious nectar and robbed the cooler, drinking 4 of those gallons of root beer.  On the second day, his wife Barbie also discovered the root beer and accidentally spilled 7 gallons. On the third day, Fred's friend Ronnie visited Fred's house and helped himself to the root beer, further reducing the amount remaining by 5 gallons.  On the fourth day, 3 people showed up for the party.  If Fred and the others shared the remaining root beer equally, how much was available for each to drink during the party?\\nAnswer:\"\n",
    "# input_text = \"Question: A farmer is buying feed for his horses. He buys a variety of hay, oats, carrots and sugar cubes. Since sugar cubes are a rare treat, he only buys two 1-pound boxes of them for the whole stable. He only wants enough carrots to feed the horses while the vegetables are fresh, so he buys four 12-pound bags. Hay is the main diet of his horses, so he buys forty-two 75-pound bales. Oats are a staple to supplement the hay, so he buys twenty 65-pound sacks. If his farm truck can carry 2250 pounds at a time, how many trips does the farmer need to transport all the feed?\\nAnswer:\"\n",
    "# input_text = \"Question: The great dragon, Perg, sat high atop mount Farbo, breathing fire upon anything within a distance of 1000 feet.  Polly could throw the gold javelin, the only known weapon that could sleigh the dragon, for a distance of 400 feet, well within the reach of the dragon's flames.  But when Polly held the sapphire gemstone, she could throw the javelin three times farther than when not holding the gemstone. If holding the gemstone, how far outside of the reach of the dragon's flames could Polly stand and still hit the dragon with the gold javelin?\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Question: Fred was preparing for a party to be held in four days.  So, he made 24 gallons of root beer on the first day and put them in the refrigerator cooler.  But later that evening, his children discovered the delicious nectar and robbed the cooler, drinking 4 of those gallons of root beer.  On the second day, his wife Barbie also discovered the root beer and accidentally spilled 7 gallons. On the third day, Fred's friend Ronnie visited Fred's house and helped himself to the root beer, further reducing the amount remaining by 5 gallons.  On the fourth day, 3 people showed up for the party.  If Fred and the others shared the remaining root beer equally, how much was available for each to drink during the party?\n",
      "Answer:\n",
      "Tokenized Input: [14582, 25, 27488, 572, 20045, 369, 264, 4614, 311, 387, 5644, 304, 3040, 2849, 13, 220, 2055, 11, 566, 1865, 220, 17, 19, 50122, 315, 3704, 12883, 389, 279, 1156, 1899, 323, 2182, 1105, 304, 279, 44944, 35821, 13, 220, 1988, 2937, 429, 11458, 11, 806, 2841, 11105, 279, 17923, 308, 38816, 323, 62254, 279, 35821, 11, 16163, 220, 19, 315, 1846, 50122, 315, 3704, 12883, 13, 220, 1913, 279, 2086, 1899, 11, 806, 7403, 83239, 1083, 11105, 279, 3704, 12883, 323, 32384, 73025, 220, 22, 50122, 13, 1913, 279, 4843, 1899, 11, 27488, 594, 4238, 83705, 11994, 27488, 594, 3753, 323, 8910, 5561, 311, 279, 3704, 12883, 11, 4623, 17719, 279, 3311, 9664, 553, 220, 20, 50122, 13, 220, 1913, 279, 11737, 1899, 11, 220, 18, 1251, 8542, 705, 369, 279, 4614, 13, 220, 1416, 27488, 323, 279, 3800, 6094, 279, 9664, 3704, 12883, 18308, 11, 1246, 1753, 572, 2500, 369, 1817, 311, 7027, 2337, 279, 4614, 5267, 16141, 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-13 19:44:53 TP0] Prefill batch. #new-seq: 1, #new-token: 159, #cached-token: 2, cache hit rate: 23.68%, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " On the first day, Fred made 24 gallons of root beer.\n",
      "\n",
      "On the second day, his children drank 4 gallons, so the remaining root beer is 24 - 4 \n",
      "[-1.4366456270217896, -0.003972123842686415, -0.006186268758028746, -0.0004194066277705133, -0.018134357407689095, -0.0017910535680130124, -0.01839544251561165, -0.004182430449873209, -7.057438779156655e-05, -2.4557415599701926e-05, -0.00026271218666806817, -0.00045506577589549124, -5.9666028391802683e-05, -0.00026533546042628586, -2.6848747730255127, -0.1333376169204712, -0.010098914615809917, -0.33787107467651367, -0.014139062725007534, -0.0005829244619235396, -0.09714391827583313, -0.005073042120784521, -0.007704236079007387, -0.0008753195870667696, -0.000451964937383309, -0.0023076317738741636, -0.24833355844020844, -0.03332608938217163, -0.4824979901313782, -0.3137463331222534, -0.6058728098869324, -5.155934559297748e-05, -0.7792341709136963, -0.16559432446956635, -9.298368240706623e-06, -2.3841887468734058e-06, -0.006599998567253351, -1.120573597290786e-05, -6.7949526965094265e-06, -2.8729851692332886e-05]\n",
      "{'text': ' On the first day, Fred made 24 gallons of root beer.\\n\\nOn the second day, his children drank 4 gallons, so the remaining root beer is 24 - 4 ', 'meta_info': {'id': '3c031e2284e44a99a33660e6eed975ed', 'finish_reason': {'type': 'stop', 'matched': '='}, 'prompt_tokens': 161, 'input_token_logprobs': [], 'output_token_logprobs': [(-1.4366456270217896, 1913, None), (-0.003972123842686415, 279, None), (-0.006186268758028746, 1156, None), (-0.0004194066277705133, 1899, None), (-0.018134357407689095, 11, None), (-0.0017910535680130124, 27488, None), (-0.01839544251561165, 1865, None), (-0.004182430449873209, 220, None), (-7.057438779156655e-05, 17, None), (-2.4557415599701926e-05, 19, None), (-0.00026271218666806817, 50122, None), (-0.00045506577589549124, 315, None), (-5.9666028391802683e-05, 3704, None), (-0.00026533546042628586, 12883, None), (-2.6848747730255127, 382, None), (-0.1333376169204712, 1925, None), (-0.010098914615809917, 279, None), (-0.33787107467651367, 2086, None), (-0.014139062725007534, 1899, None), (-0.0005829244619235396, 11, None), (-0.09714391827583313, 806, None), (-0.005073042120784521, 2841, None), (-0.007704236079007387, 53144, None), (-0.0008753195870667696, 220, None), (-0.000451964937383309, 19, None), (-0.0023076317738741636, 50122, None), (-0.24833355844020844, 11, None), (-0.03332608938217163, 773, None), (-0.4824979901313782, 279, None), (-0.3137463331222534, 9664, None), (-0.6058728098869324, 3704, None), (-5.155934559297748e-05, 12883, None), (-0.7792341709136963, 374, None), (-0.16559432446956635, 220, None), (-9.298368240706623e-06, 17, None), (-2.3841887468734058e-06, 19, None), (-0.006599998567253351, 481, None), (-1.120573597290786e-05, 220, None), (-6.7949526965094265e-06, 19, None), (-2.8729851692332886e-05, 284, None)], 'completion_tokens': 40, 'cached_tokens': 2}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-13 19:44:53 TP0] Decode batch. #running-req: 1, #token: 189, token usage: 0.00, gen throughput (token/s): 0.61, #queue-req: 0\n"
     ]
    }
   ],
   "source": [
    "input_tokens = sgl_tokenizer.encode(input_text)\n",
    "print(f\"Input Text: {input_text}\")\n",
    "print(f\"Tokenized Input: {input_tokens}\")\n",
    "outputs = sgl_llm.generate(input_ids=input_tokens, sampling_params=sgl_sampling_params, return_logprob=True)\n",
    "meta_info = outputs[\"meta_info\"]\n",
    "output_token_logprobs = meta_info[\"output_token_logprobs\"]\n",
    "logprobs = [output[0] for output in output_token_logprobs]\n",
    "print(outputs['text'])\n",
    "print(logprobs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s, est. speed input: 420.44 toks/s, output: 78.34 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input: [14582, 25, 27488, 572, 20045, 369, 264, 4614, 311, 387, 5644, 304, 3040, 2849, 13, 220, 2055, 11, 566, 1865, 220, 17, 19, 50122, 315, 3704, 12883, 389, 279, 1156, 1899, 323, 2182, 1105, 304, 279, 44944, 35821, 13, 220, 1988, 2937, 429, 11458, 11, 806, 2841, 11105, 279, 17923, 308, 38816, 323, 62254, 279, 35821, 11, 16163, 220, 19, 315, 1846, 50122, 315, 3704, 12883, 13, 220, 1913, 279, 2086, 1899, 11, 806, 7403, 83239, 1083, 11105, 279, 3704, 12883, 323, 32384, 73025, 220, 22, 50122, 13, 1913, 279, 4843, 1899, 11, 27488, 594, 4238, 83705, 11994, 27488, 594, 3753, 323, 8910, 5561, 311, 279, 3704, 12883, 11, 4623, 17719, 279, 3311, 9664, 553, 220, 20, 50122, 13, 220, 1913, 279, 11737, 1899, 11, 220, 18, 1251, 8542, 705, 369, 279, 4614, 13, 220, 1416, 27488, 323, 279, 3800, 6094, 279, 9664, 3704, 12883, 18308, 11, 1246, 1753, 572, 2500, 369, 1817, 311, 7027, 2337, 279, 4614, 5267, 16141, 25]\n",
      "[' Start of the episode you had 24 gallons of root beer.  Children drink your nectar and reduce it to 24-4']\n",
      "[-7.548278331756592, -4.167039394378662, -0.6483224630355835, -9.1777982711792, -4.570546627044678, -1.5898911952972412, -0.08874617516994476, -0.0014422263484448195, -0.0004493180604185909, -0.017425768077373505, -0.8181905150413513, -0.17172014713287354, -0.027506733313202858, -2.274561882019043, -0.6878971457481384, -1.8176482915878296, -2.410999059677124, -5.506723403930664, -2.6896984577178955, -0.001069331425242126, -1.58378005027771, -2.458317518234253, -1.7114146947860718, -1.0549685955047607, -0.05280338227748871, -0.008807620033621788, -0.030355654656887054, -0.7484126091003418, -0.047567419707775116, -0.8500243425369263]\n",
      "RequestOutput(request_id=2, prompt=None, prompt_token_ids=[14582, 25, 27488, 572, 20045, 369, 264, 4614, 311, 387, 5644, 304, 3040, 2849, 13, 220, 2055, 11, 566, 1865, 220, 17, 19, 50122, 315, 3704, 12883, 389, 279, 1156, 1899, 323, 2182, 1105, 304, 279, 44944, 35821, 13, 220, 1988, 2937, 429, 11458, 11, 806, 2841, 11105, 279, 17923, 308, 38816, 323, 62254, 279, 35821, 11, 16163, 220, 19, 315, 1846, 50122, 315, 3704, 12883, 13, 220, 1913, 279, 2086, 1899, 11, 806, 7403, 83239, 1083, 11105, 279, 3704, 12883, 323, 32384, 73025, 220, 22, 50122, 13, 1913, 279, 4843, 1899, 11, 27488, 594, 4238, 83705, 11994, 27488, 594, 3753, 323, 8910, 5561, 311, 279, 3704, 12883, 11, 4623, 17719, 279, 3311, 9664, 553, 220, 20, 50122, 13, 220, 1913, 279, 11737, 1899, 11, 220, 18, 1251, 8542, 705, 369, 279, 4614, 13, 220, 1416, 27488, 323, 279, 3800, 6094, 279, 9664, 3704, 12883, 18308, 11, 1246, 1753, 572, 2500, 369, 1817, 311, 7027, 2337, 279, 4614, 5267, 16141, 25], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Start of the episode you had 24 gallons of root beer.  Children drink your nectar and reduce it to 24-4', token_ids=(5145, 315, 279, 9234, 498, 1030, 220, 17, 19, 50122, 315, 3704, 12883, 13, 220, 15044, 7027, 697, 308, 38816, 323, 7949, 432, 311, 220, 17, 19, 12, 19, 28), cumulative_logprob=-52.76240662505734, logprobs=[{5145: Logprob(logprob=-7.548278331756592, rank=31, decoded_token=' Start')}, {315: Logprob(logprob=-4.167039394378662, rank=5, decoded_token=' of')}, {279: Logprob(logprob=-0.6483224630355835, rank=1, decoded_token=' the')}, {9234: Logprob(logprob=-9.1777982711792, rank=159, decoded_token=' episode')}, {498: Logprob(logprob=-4.570546627044678, rank=13, decoded_token=' you')}, {1030: Logprob(logprob=-1.5898911952972412, rank=2, decoded_token=' had')}, {220: Logprob(logprob=-0.08874617516994476, rank=1, decoded_token=' ')}, {17: Logprob(logprob=-0.0014422263484448195, rank=1, decoded_token='2')}, {19: Logprob(logprob=-0.0004493180604185909, rank=1, decoded_token='4')}, {50122: Logprob(logprob=-0.017425768077373505, rank=1, decoded_token=' gallons')}, {315: Logprob(logprob=-0.8181905150413513, rank=1, decoded_token=' of')}, {3704: Logprob(logprob=-0.17172014713287354, rank=1, decoded_token=' root')}, {12883: Logprob(logprob=-0.027506733313202858, rank=1, decoded_token=' beer')}, {13: Logprob(logprob=-2.274561882019043, rank=4, decoded_token='.')}, {220: Logprob(logprob=-0.6878971457481384, rank=1, decoded_token=' ')}, {15044: Logprob(logprob=-1.8176482915878296, rank=1, decoded_token=' Children')}, {7027: Logprob(logprob=-2.410999059677124, rank=2, decoded_token=' drink')}, {697: Logprob(logprob=-5.506723403930664, rank=7, decoded_token=' your')}, {308: Logprob(logprob=-2.6896984577178955, rank=5, decoded_token=' n')}, {38816: Logprob(logprob=-0.001069331425242126, rank=1, decoded_token='ectar')}, {323: Logprob(logprob=-1.58378005027771, rank=1, decoded_token=' and')}, {7949: Logprob(logprob=-2.458317518234253, rank=3, decoded_token=' reduce')}, {432: Logprob(logprob=-1.7114146947860718, rank=2, decoded_token=' it')}, {311: Logprob(logprob=-1.0549685955047607, rank=2, decoded_token=' to')}, {220: Logprob(logprob=-0.05280338227748871, rank=1, decoded_token=' ')}, {17: Logprob(logprob=-0.008807620033621788, rank=1, decoded_token='2')}, {19: Logprob(logprob=-0.030355654656887054, rank=1, decoded_token='4')}, {12: Logprob(logprob=-0.7484126091003418, rank=1, decoded_token='-')}, {19: Logprob(logprob=-0.047567419707775116, rank=1, decoded_token='4')}, {28: Logprob(logprob=-0.8500243425369263, rank=1, decoded_token='=')}], finish_reason=stop, stop_reason==)], finished=True, metrics=RequestMetrics(arrival_time=1739475900.0247824, last_token_time=1739475900.3982437, first_scheduled_time=1739475900.0267699, first_token_time=1739475900.0492334, time_in_queue=0.001987457275390625, finished_time=1739475900.398394, scheduler_time=0.0033309897407889366, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_tokens = vllm_tokenizer.encode(input_text)\n",
    "vllm_output = vllm_llm.generate(prompt_token_ids=input_tokens,\n",
    "                sampling_params=vllm_sampling_params)\n",
    "vllm_output = vllm_output[0]\n",
    "output_token_ids = [outputs.token_ids for outputs in vllm_output.outputs]\n",
    "logprobs = [output.logprobs for output in vllm_output.outputs] or []\n",
    "logprobs = [logprob[token_id].logprob for token_id, logprob in zip(output_token_ids[0], logprobs[0])]\n",
    "result = [output.text for output in vllm_output.outputs]\n",
    "input_token_ids = vllm_output.prompt_token_ids\n",
    "print(f\"Tokenized Input: {input_tokens}\")\n",
    "print(result)\n",
    "print(logprobs)\n",
    "print(vllm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```python\n",
      "def remaining_root_beverage():\n",
      "    \"\"\"Fred was preparing for a party to be held in four days.  So, he made 24 gallons of root beer on the first day and put them in the refrigerator cooler.  But later that evening, his children discovered the delicious nectar and robbed the cooler, drinking 4 of those gallons of root beer.  On the second day, his wife Barbie also discovered the root beer and accidentally spilled 7 gallons. On the third day, Fred's friend Ronnie visited Fred's house and helped himself to the root beer, further reducing the amount remaining by 5 gallons.  On the fourth day, 3 people showed up for the party.  If Fred and the others shared the remaining root beer equally, how much was available for each to drink during the party?\"\"\"\n",
      "    root_beverage_initial =\n",
      "[-0.746602475643158, -5.483612312673358e-06, -0.003201955696567893, -0.6629713773727417, -0.6062862277030945, -0.0010827876394614577, -0.603727400302887, -0.7373879551887512, -0.0008985534077510238, -0.0012768696760758758, -2.1457441107486375e-05, -0.0022722873836755753, -0.0003935516288038343, -8.844937838148326e-05, -3.611976353568025e-05, -1.680836794548668e-05, -4.529942543740617e-06, -1.0490362910786644e-05, -7.510157047363464e-06, -2.264974000354414e-06, -3.933898824470816e-06, -0.00011455356434453279, -6.258291978156194e-05, -7.986990567587782e-06, -1.9788545614574105e-05, -0.003210511291399598, -7.748573807475623e-06, -2.50339189733495e-06, -1.6689160474925302e-05, -1.585470999998506e-05, -1.8000440832111053e-05, -2.3841830625315197e-06, -1.7881377516459906e-06, -6.079655122448457e-06, -8.940656698541716e-06, -1.3589766240329482e-05, -0.00015615197480656207, -7.152531907195225e-06, -8.940656698541716e-06, -1.9192511899746023e-05, -9.65590606938349e-06, -2.455681169521995e-05, -6.9141146923357155e-06, -0.0001370812824461609, -1.0847986231965479e-05, -1.3112935448589269e-05, -0.000447530735982582, -0.0002252801787108183, -9.894321920000948e-06, -9.179073458653875e-06, -2.3841830625315197e-06, -6.794906312279636e-06, -9.059865078597795e-06, -1.5497195136049413e-06, -1.2040065485052764e-05, -7.867782187531702e-06, -1.966933996300213e-05, -6.878139538457617e-05, -5.8412379075889476e-06, -2.7179348762729205e-05, -5.781483559985645e-05, -1.1920928244535389e-07, -4.172316494077677e-06, -0.00039057256071828306, -2.5987286790041253e-05, -3.2543604902457446e-05, -1.2040065485052764e-05, -2.038458114839159e-05, -3.933898824470816e-06, -1.156323378381785e-05, -0.008643240667879581, -2.753696753643453e-05, -1.7881377516459906e-06, -0.00010048838157672435, -5.006777428206988e-06, -5.61460001335945e-05, -1.4066597032069694e-05, -0.00011753345461329445, -4.291525328881107e-06, -5.960462772236497e-07, -8.34461570775602e-06, -3.933898824470816e-06, -2.9802276912960224e-06, -3.611976353568025e-05, -1.0967194612021558e-05, -0.0005118728731758893, -8.49926145747304e-05, -1.2397689715726301e-05, -8.106198947643861e-06, -7.557583012385294e-05, -3.158996332786046e-05, -8.106198947643861e-06, -0.0001770101225702092, -0.002301426837220788, -1.9788545614574105e-05, -5.8412379075889476e-06, -5.245195097813848e-06, -0.0002361257211305201, -6.723177648382261e-05, -8.344646857949556e-07, -3.4450891689630225e-05, -3.6954811548639555e-06, -2.0265558760002023e-06, -6.19869097135961e-05, -0.00012599628826137632, -2.062299427052494e-05, -2.2172682292875834e-05, -7.712543447269127e-05, -6.758938252460212e-05, -1.2874520507466514e-05, -7.271740287251305e-06, -3.576272320060525e-06, -0.00011932138295378536, -1.4066597032069694e-05, -3.576272320060525e-06, -1.07287787614041e-05, -4.9232225137529895e-05, -0.00019524575327523053, -9.894321920000948e-06, -0.00012468514614738524, -2.407998726994265e-05, -5.602820692729438e-06, -0.0001760566228767857, -0.0008314966107718647, -1.4543427823809907e-05, -2.622600959512056e-06, -1.0847986231965479e-05, -1.6689286894688848e-06, -1.8715683836489916e-05, -9.047575440490618e-05, -6.079655122448457e-06, -4.768370445162873e-07, -5.8887653722194955e-05, -1.5735502529423684e-05, -4.291525328881107e-06, -3.4689302992774174e-05, -1.3589766240329482e-05, -6.496695277746767e-05, -0.00017391123401466757, -3.814689989667386e-06, -2.1457441107486375e-05, -3.814689989667386e-06, -2.7656173188006505e-05, -0.0001248043408850208, -3.290122185717337e-05, -0.0005645350320264697, -1.7762025890988298e-05, -4.649054244509898e-05, -1.2397689715726301e-05, -5.352353764465079e-05, -2.2172682292875834e-05, -2.264974000354414e-06, -1.0847986231965479e-05, -3.3378044463461265e-05, -8.165503095369786e-05, -5.364403477869928e-06, -0.00010656742961145937, -3.6477376852417365e-05, -6.318072337307967e-06, -6.41325386823155e-05, -1.8954096958623268e-05, -1.1324817933200393e-05, -1.7881377516459906e-06, -4.649054244509898e-05, -3.755022044060752e-05, -3.4689302992774174e-05, -1.9073468138230965e-06, -0.00010561384988250211, -3.0278701160568744e-05, -0.0003300360403954983, -4.160317621426657e-05, -0.05087558180093765, -0.03486672043800354, -0.005332531873136759, -0.001142445020377636, -0.008694176562130451, -0.00046004203613847494]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\"\n",
    "model_inputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n",
    "outputs = llm.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=256,\n",
    "    stop_strings=['Question=', 'Question', '=', '<|endoftext|>'],\n",
    "    tokenizer=tokenizer,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, outputs.sequences)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "\n",
    "log_probs = []\n",
    "for step, step_logits in enumerate(outputs.scores):\n",
    "    token_id = generated_ids[0][step]\n",
    "    step_log_probs = F.log_softmax(step_logits, dim=-1)\n",
    "    token_log_prob = step_log_probs[:, token_id].item()\n",
    "    log_probs.append(token_log_prob)\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/loky-2586179-tytf41c3'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
