# Use SGLang as backend

Lighteval allows you to use `sglang` as backend allowing great speedups.
To use, simply change the `model_args` to reflect the arguments you want to pass to sglang.

```bash
lighteval sglang \
    "pretrained=HuggingFaceH4/zephyr-7b-beta,dtype=float16" \
    "leaderboard|truthfulqa:mc|0|0"
```

`sglang` is able to distribute the model across multiple GPUs using data
parallelism and tensor parallelism.
You can choose the parallelism method by setting in the the `model_args`.

For example if you have 4 GPUs you can split it across using `tp_size`:

```bash
lighteval sglang \
    "pretrained=HuggingFaceH4/zephyr-7b-beta,dtype=float16,tp_size=4" \
    "leaderboard|truthfulqa:mc|0|0"
```

Or, if your model fits on a single GPU, you can use `dp_size` to speed up the evaluation:

```bash
lighteval sglang \
    "pretrained=HuggingFaceH4/zephyr-7b-beta,dtype=float16,dp_size=4" \
    "leaderboard|truthfulqa:mc|0|0"
```

Available arguments for `sglang` can be found in the `SGLangModelConfig`:

- **pretrained** (str): HuggingFace Hub model ID name or the path to a pre-trained model to load.
- **load_format** (str): The format the weights are loaded in. Defaults to *.safetensors/*.bin.
- **dtype** (str): Dtype used for the model, defaults to bfloat16.
- **tp_size** (int): The number of GPUs the model weights get sharded over.
- **dp_size** (int): The number of data-parallel copies of the model.
- **context_length** (int | None): The number of tokens our model can process including the input.
- **random_seed** (int): Can be used to enforce more deterministic behavior.
- **trust_remote_code** (bool): If True, will use locally cached config files, otherwise use remote configs in HuggingFace.
- **skip_tokenizer_init** (bool): Set to true to provide the tokens to the engine and get the output tokens directly, typically used in RLHF.
- **kv_cache_dtype** (str): Dtype of the kv cache, defaults to the auto.
- **add_special_tokens** (bool): Whether to add special tokens to the input sequences.
- **sampling_backend** (str | None): The backend for sampling.
- **attention_backend** (str | None): The backend for attention computation and KV cache management.
- **mem_fraction_static** (float): Fraction of the free GPU memory used for static memory like model weights and KV cache.
- **chunked_prefill_size** (int): Perform the prefill in chunks of these size.

> [!WARNING]
> In the case of OOM issues, you might need to reduce the context size of the
> model as well as reduce the `mem_fraction_static` and `chunked_prefill_size` parameter.
