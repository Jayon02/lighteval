{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/qiujiang/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 18:38:07 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:38:07,649\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-13 18:38:09 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-13 18:38:14 config.py:542] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 02-13 18:38:14 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-13 18:38:14 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 02-13 18:38:14 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2-7B', speculative_config=None, tokenizer='Qwen/Qwen2-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config=None, tokenizer_revision=main, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=Qwen/Qwen2-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-13 18:38:16 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-13 18:38:17 model_runner.py:1110] Starting to load model Qwen/Qwen2-7B...\n",
      "INFO 02-13 18:38:17 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.07s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.03s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.06s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.06s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 18:38:22 model_runner.py:1115] Loading model weights took 14.2717 GB\n",
      "INFO 02-13 18:38:23 worker.py:267] Memory profiling takes 0.66 seconds\n",
      "INFO 02-13 18:38:23 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.80) = 63.40GiB\n",
      "INFO 02-13 18:38:23 worker.py:267] model weights take 14.27GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 47.64GiB.\n",
      "INFO 02-13 18:38:23 executor_base.py:110] # CUDA blocks: 55749, # CPU blocks: 4681\n",
      "INFO 02-13 18:38:23 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 6.81x\n",
      "INFO 02-13 18:38:25 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:24<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 18:38:50 model_runner.py:1562] Graph capturing finished in 24 secs, took 0.19 GiB\n",
      "INFO 02-13 18:38:50 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 27.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "model_args = {\n",
    "            'model': 'Qwen/Qwen2-7B',\n",
    "            'gpu_memory_utilization': 0.8,\n",
    "            'trust_remote_code': False, \n",
    "            'revision': 'main',\n",
    "            'dtype': 'float16', \n",
    "            'seed': 1234,\n",
    "            'max_model_len': None, \n",
    "            'tensor_parallel_size': 1, \n",
    "            'pipeline_parallel_size': 1, \n",
    "            'swap_space': 4, \n",
    "        }\n",
    "\n",
    "llm = LLM(**model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 193.94 toks/s, output: 79.34 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input: [14582, 25, 576, 2244, 25105, 11, 393, 2375, 11, 7578, 1550, 45988, 6470, 13452, 749, 11, 25938, 3940, 5193, 4113, 2878, 264, 6010, 315, 220, 16, 15, 15, 15, 7541, 13, 220, 93417, 1410, 2510, 279, 6623, 1258, 32730, 11, 279, 1172, 3881, 10288, 429, 1410, 19747, 1090, 279, 25105, 11, 369, 264, 6010, 315, 220, 19, 15, 15, 7541, 11, 1632, 2878, 279, 5545, 315, 279, 25105, 594, 38785, 13, 220, 1988, 979, 93417, 5644, 279, 274, 55105, 18747, 10812, 11, 1340, 1410, 2510, 279, 1258, 32730, 2326, 3039, 42626, 1091, 979, 537, 9963, 279, 18747, 10812, 13, 1416, 9963, 279, 18747, 10812, 11, 1246, 3041, 4889, 315, 279, 5545, 315, 279, 25105, 594, 38785, 1410, 93417, 2498, 323, 2058, 4201, 279, 25105, 448, 279, 6623, 1258, 32730, 5267, 16141, 25]\n",
      "[' When Polly is not holding the gemstone, she can throw the javelin 400 feet. \\nHowever, when she holds the gemstone, she can throw the javelin three times farther, so that would be 400 feet * 3 ']\n",
      "[-1.498100757598877, -0.1620449423789978, -0.6088792085647583, -0.06354980170726776, -0.000802075956016779, -0.0007613382767885923, -0.37802886962890625, -0.0008276851149275899, -0.006247159093618393, -0.07465990632772446, -0.0018421123968437314, -0.004208873957395554, -0.00034731553751043975, -0.14782148599624634, -0.00012706902634818107, -1.0982251167297363, -0.0024239225313067436, -2.5629668016335927e-05, -1.2993727978027891e-05, -0.0008878341759555042, -1.102305293083191, -1.9600915908813477, -3.105787754058838, -7.748573807475623e-06, -0.034834831953048706, -0.02052975445985794, -0.265521764755249, -5.2927523938706145e-05, -0.36779195070266724, -2.932505594799295e-05, -0.00014768941036891192, -0.007277053315192461, -0.00019369633810129017, -9.881961887003854e-05, -0.028338275849819183, -0.00018189683032687753, -7.152531907195225e-06, -0.05706535279750824, -6.794906312279636e-06, -0.002213291823863983, -0.19876235723495483, -0.5935291051864624, -2.2292869091033936, -1.1604981422424316, -0.0011312521528452635, -0.03463241457939148, -0.10327570885419846, -6.09140915912576e-05, -2.3841855067985307e-07, -1.3175632953643799, -1.561429738998413, -4.6132929128361866e-05, -1.8358061424805783e-05, -0.00015853578224778175]\n",
      "RequestOutput(request_id=13, prompt=None, prompt_token_ids=[14582, 25, 576, 2244, 25105, 11, 393, 2375, 11, 7578, 1550, 45988, 6470, 13452, 749, 11, 25938, 3940, 5193, 4113, 2878, 264, 6010, 315, 220, 16, 15, 15, 15, 7541, 13, 220, 93417, 1410, 2510, 279, 6623, 1258, 32730, 11, 279, 1172, 3881, 10288, 429, 1410, 19747, 1090, 279, 25105, 11, 369, 264, 6010, 315, 220, 19, 15, 15, 7541, 11, 1632, 2878, 279, 5545, 315, 279, 25105, 594, 38785, 13, 220, 1988, 979, 93417, 5644, 279, 274, 55105, 18747, 10812, 11, 1340, 1410, 2510, 279, 1258, 32730, 2326, 3039, 42626, 1091, 979, 537, 9963, 279, 18747, 10812, 13, 1416, 9963, 279, 18747, 10812, 11, 1246, 3041, 4889, 315, 279, 5545, 315, 279, 25105, 594, 38785, 1410, 93417, 2498, 323, 2058, 4201, 279, 25105, 448, 279, 6623, 1258, 32730, 5267, 16141, 25], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' When Polly is not holding the gemstone, she can throw the javelin 400 feet. \\nHowever, when she holds the gemstone, she can throw the javelin three times farther, so that would be 400 feet * 3 ', token_ids=(3197, 93417, 374, 537, 9963, 279, 18747, 10812, 11, 1340, 646, 2510, 279, 1258, 32730, 220, 19, 15, 15, 7541, 13, 715, 11209, 11, 979, 1340, 9982, 279, 18747, 10812, 11, 1340, 646, 2510, 279, 1258, 32730, 2326, 3039, 42626, 11, 773, 429, 1035, 387, 220, 19, 15, 15, 7541, 353, 220, 18, 284), cumulative_logprob=-18.202700165864798, logprobs=[{3197: Logprob(logprob=-1.498100757598877, rank=2, decoded_token=' When')}, {93417: Logprob(logprob=-0.1620449423789978, rank=1, decoded_token=' Polly')}, {374: Logprob(logprob=-0.6088792085647583, rank=1, decoded_token=' is')}, {537: Logprob(logprob=-0.06354980170726776, rank=1, decoded_token=' not')}, {9963: Logprob(logprob=-0.000802075956016779, rank=1, decoded_token=' holding')}, {279: Logprob(logprob=-0.0007613382767885923, rank=1, decoded_token=' the')}, {18747: Logprob(logprob=-0.37802886962890625, rank=1, decoded_token=' gem')}, {10812: Logprob(logprob=-0.0008276851149275899, rank=1, decoded_token='stone')}, {11: Logprob(logprob=-0.006247159093618393, rank=1, decoded_token=',')}, {1340: Logprob(logprob=-0.07465990632772446, rank=1, decoded_token=' she')}, {646: Logprob(logprob=-0.0018421123968437314, rank=1, decoded_token=' can')}, {2510: Logprob(logprob=-0.004208873957395554, rank=1, decoded_token=' throw')}, {279: Logprob(logprob=-0.00034731553751043975, rank=1, decoded_token=' the')}, {1258: Logprob(logprob=-0.14782148599624634, rank=1, decoded_token=' jav')}, {32730: Logprob(logprob=-0.00012706902634818107, rank=1, decoded_token='elin')}, {220: Logprob(logprob=-1.0982251167297363, rank=2, decoded_token=' ')}, {19: Logprob(logprob=-0.0024239225313067436, rank=1, decoded_token='4')}, {15: Logprob(logprob=-2.5629668016335927e-05, rank=1, decoded_token='0')}, {15: Logprob(logprob=-1.2993727978027891e-05, rank=1, decoded_token='0')}, {7541: Logprob(logprob=-0.0008878341759555042, rank=1, decoded_token=' feet')}, {13: Logprob(logprob=-1.102305293083191, rank=2, decoded_token='.')}, {715: Logprob(logprob=-1.9600915908813477, rank=2, decoded_token=' \\n')}, {11209: Logprob(logprob=-3.105787754058838, rank=5, decoded_token='However')}, {11: Logprob(logprob=-7.748573807475623e-06, rank=1, decoded_token=',')}, {979: Logprob(logprob=-0.034834831953048706, rank=1, decoded_token=' when')}, {1340: Logprob(logprob=-0.02052975445985794, rank=1, decoded_token=' she')}, {9982: Logprob(logprob=-0.265521764755249, rank=1, decoded_token=' holds')}, {279: Logprob(logprob=-5.2927523938706145e-05, rank=1, decoded_token=' the')}, {18747: Logprob(logprob=-0.36779195070266724, rank=1, decoded_token=' gem')}, {10812: Logprob(logprob=-2.932505594799295e-05, rank=1, decoded_token='stone')}, {11: Logprob(logprob=-0.00014768941036891192, rank=1, decoded_token=',')}, {1340: Logprob(logprob=-0.007277053315192461, rank=1, decoded_token=' she')}, {646: Logprob(logprob=-0.00019369633810129017, rank=1, decoded_token=' can')}, {2510: Logprob(logprob=-9.881961887003854e-05, rank=1, decoded_token=' throw')}, {279: Logprob(logprob=-0.028338275849819183, rank=1, decoded_token=' the')}, {1258: Logprob(logprob=-0.00018189683032687753, rank=1, decoded_token=' jav')}, {32730: Logprob(logprob=-7.152531907195225e-06, rank=1, decoded_token='elin')}, {2326: Logprob(logprob=-0.05706535279750824, rank=1, decoded_token=' three')}, {3039: Logprob(logprob=-6.794906312279636e-06, rank=1, decoded_token=' times')}, {42626: Logprob(logprob=-0.002213291823863983, rank=1, decoded_token=' farther')}, {11: Logprob(logprob=-0.19876235723495483, rank=1, decoded_token=',')}, {773: Logprob(logprob=-0.5935291051864624, rank=1, decoded_token=' so')}, {429: Logprob(logprob=-2.2292869091033936, rank=3, decoded_token=' that')}, {1035: Logprob(logprob=-1.1604981422424316, rank=2, decoded_token=' would')}, {387: Logprob(logprob=-0.0011312521528452635, rank=1, decoded_token=' be')}, {220: Logprob(logprob=-0.03463241457939148, rank=1, decoded_token=' ')}, {19: Logprob(logprob=-0.10327570885419846, rank=1, decoded_token='4')}, {15: Logprob(logprob=-6.09140915912576e-05, rank=1, decoded_token='0')}, {15: Logprob(logprob=-2.3841855067985307e-07, rank=1, decoded_token='0')}, {7541: Logprob(logprob=-1.3175632953643799, rank=3, decoded_token=' feet')}, {353: Logprob(logprob=-1.561429738998413, rank=2, decoded_token=' *')}, {220: Logprob(logprob=-4.6132929128361866e-05, rank=1, decoded_token=' ')}, {18: Logprob(logprob=-1.8358061424805783e-05, rank=1, decoded_token='3')}, {284: Logprob(logprob=-0.00015853578224778175, rank=1, decoded_token=' =')}], finish_reason=stop, stop_reason==)], finished=True, metrics=RequestMetrics(arrival_time=1739472780.5308185, last_token_time=1739472781.202098, first_scheduled_time=1739472780.5329728, first_token_time=1739472780.5630033, time_in_queue=0.0021543502807617188, finished_time=1739472781.2022605, scheduler_time=0.0056828698143363, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "from vllm.transformers_utils.tokenizer import get_tokenizer\n",
    "\n",
    "vllm_tokenizer = get_tokenizer(\n",
    "               \"Qwen/Qwen2-7B\",\n",
    "               tokenizer_mode=\"auto\",\n",
    "               trust_remote_code=False,\n",
    "               tokenizer_revision=\"main\",\n",
    "            )\n",
    "# 这个text让sglang第二次生成就基本上对齐\n",
    "# input_text = \"Question: Fred was preparing for a party to be held in four days.  So, he made 24 gallons of root beer on the first day and put them in the refrigerator cooler.  But later that evening, his children discovered the delicious nectar and robbed the cooler, drinking 4 of those gallons of root beer.  On the second day, his wife Barbie also discovered the root beer and accidentally spilled 7 gallons. On the third day, Fred's friend Ronnie visited Fred's house and helped himself to the root beer, further reducing the amount remaining by 5 gallons.  On the fourth day, 3 people showed up for the party.  If Fred and the others shared the remaining root beer equally, how much was available for each to drink during the party?\\nAnswer:\"\n",
    "# 这个text，sglang和vllm生成一次就对齐了\n",
    "# input_text = \"Question: A farmer is buying feed for his horses. He buys a variety of hay, oats, carrots and sugar cubes. Since sugar cubes are a rare treat, he only buys two 1-pound boxes of them for the whole stable. He only wants enough carrots to feed the horses while the vegetables are fresh, so he buys four 12-pound bags. Hay is the main diet of his horses, so he buys forty-two 75-pound bales. Oats are a staple to supplement the hay, so he buys twenty 65-pound sacks. If his farm truck can carry 2250 pounds at a time, how many trips does the farmer need to transport all the feed?\\nAnswer:\"\n",
    "# 这个text对齐次数一直在变，但是也有齐的时候\n",
    "input_text = \"Question: The great dragon, Perg, sat high atop mount Farbo, breathing fire upon anything within a distance of 1000 feet.  Polly could throw the gold javelin, the only known weapon that could sleigh the dragon, for a distance of 400 feet, well within the reach of the dragon's flames.  But when Polly held the sapphire gemstone, she could throw the javelin three times farther than when not holding the gemstone. If holding the gemstone, how far outside of the reach of the dragon's flames could Polly stand and still hit the dragon with the gold javelin?\\nAnswer:\"\n",
    "\n",
    "input_tokens = vllm_tokenizer.encode(input_text)\n",
    "\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    n=1, \n",
    "    presence_penalty=0.0, \n",
    "    frequency_penalty=0.0, \n",
    "    repetition_penalty=1.0, \n",
    "    temperature=1.0, \n",
    "    top_p=1.0, \n",
    "    top_k=-1, \n",
    "    min_p=0.0, \n",
    "    seed=None, \n",
    "    stop=['Question=', 'Question', '=', '<|endoftext|>'], \n",
    "    stop_token_ids=[], \n",
    "    bad_words=[], \n",
    "    include_stop_str_in_output=False, \n",
    "    ignore_eos=False, \n",
    "    max_tokens=256, \n",
    "    min_tokens=0, \n",
    "    logprobs=0, \n",
    "    prompt_logprobs=None, \n",
    "    skip_special_tokens=True, \n",
    "    spaces_between_special_tokens=True, \n",
    "    truncate_prompt_tokens=None, \n",
    "    guided_decoding=None\n",
    "    )\n",
    "\n",
    "vllm_output = llm.generate(prompt_token_ids=input_tokens,\n",
    "                sampling_params=vllm_sampling_params)\n",
    "vllm_output = vllm_output[0]\n",
    "output_token_ids = [outputs.token_ids for outputs in vllm_output.outputs]\n",
    "logprobs = [output.logprobs for output in vllm_output.outputs] or []\n",
    "logprobs = [logprob[token_id].logprob for token_id, logprob in zip(output_token_ids[0], logprobs[0])]\n",
    "result = [output.text for output in vllm_output.outputs]\n",
    "input_token_ids = vllm_output.prompt_token_ids\n",
    "print(f\"Tokenized Input: {input_tokens}\")\n",
    "print(result)\n",
    "print(logprobs)\n",
    "print(vllm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# from vllm.distributed.parallel_state import destroy_distributed_environment, destroy_model_parallel\n",
    "# import torch\n",
    "# import gc\n",
    "\n",
    "# destroy_model_parallel()\n",
    "# del llm.llm_engine.model_executor.driver_worker\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
